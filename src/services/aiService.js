import OpenAI from 'openai'
import Anthropic from '@anthropic-ai/sdk'
import { GoogleGenerativeAI } from '@google/generative-ai'
import { useStore } from '../store/useStore'
import googleAuthService from './googleAuth'

// ç»Ÿä¸€çš„ AI æœåŠ¡ç±»
class AIService {
  constructor() {
    this.clients = {}
    // é»˜è®¤ endpoint
    this.defaultEndpoints = {
      openai: 'https://api.openai.com/v1',
      anthropic: 'https://api.anthropic.com',
      google: 'https://generativelanguage.googleapis.com/v1beta',
      vertex: 'https://us-central1-aiplatform.googleapis.com/v1beta',
      custom: ''
    }
  }

  normalizeProxyUrl(url) {
    if (!url) return ''
    return url.endsWith('/') ? url : `${url}/`
  }

  // è§„èŒƒåŒ– Endpointï¼Œå¤„ç†é‡å¤çš„ /v1 ç‰‡æ®µç­‰
  normalizeEndpoint(provider, endpoint) {
    if (!endpoint) return endpoint

    let normalized = endpoint.trim()

    if (provider === 'anthropic') {
      normalized = normalized.replace(/\/+v1\/?$/, '')
    }

    return normalized.replace(/\/+$/, '')
  }

  // è·å– API Keyï¼ˆç¯å¢ƒå˜é‡ä¼˜å…ˆï¼‰
  getApiKey(provider) {
    // ç¯å¢ƒå˜é‡ä¼˜å…ˆ
    const envKey = {
      openai: import.meta.env.VITE_OPENAI_API_KEY,
      anthropic: import.meta.env.VITE_ANTHROPIC_API_KEY,
      google: import.meta.env.VITE_GOOGLE_API_KEY,
      vertex: import.meta.env.VITE_VERTEX_API_KEY,
      custom: import.meta.env.VITE_CUSTOM_API_KEY
    }[provider]

    if (envKey) return envKey

    // å›é€€åˆ° localStorage
    const { providers } = useStore.getState()
    return providers?.[provider]?.apiKey
  }

  // è·å– API Endpointï¼ˆç¯å¢ƒå˜é‡ä¼˜å…ˆï¼‰
  getApiEndpoint(provider) {
    // ç¯å¢ƒå˜é‡ä¼˜å…ˆ
    const envEndpoint = {
      openai: import.meta.env.VITE_OPENAI_API_ENDPOINT,
      anthropic: import.meta.env.VITE_ANTHROPIC_API_ENDPOINT,
      google: import.meta.env.VITE_GOOGLE_API_ENDPOINT,
      vertex: import.meta.env.VITE_VERTEX_API_ENDPOINT,
      custom: import.meta.env.VITE_CUSTOM_API_ENDPOINT
    }[provider]

    if (envEndpoint) return this.normalizeEndpoint(provider, envEndpoint)

    // å›é€€åˆ°ç”¨æˆ·é…ç½®
    const { providers } = useStore.getState()
    const storedEndpoint = providers?.[provider]?.baseURL

    return this.normalizeEndpoint(provider, storedEndpoint || this.defaultEndpoints[provider])
  }

  // è·å– Vertex é…ç½®
  getVertexConfig() {
    const { providers } = useStore.getState()
    const vertexConfig = providers?.vertex || {}
    
    return {
      projectId: import.meta.env.VITE_VERTEX_PROJECT_ID || vertexConfig.projectId,
      location: import.meta.env.VITE_VERTEX_LOCATION || vertexConfig.location || 'us-central1'
    }
  }

  // åˆå§‹åŒ–å®¢æˆ·ç«¯
  initClient(provider, config = {}) {
    const resolvedApiKey = this.getApiKey(provider) || config.apiKey
    const resolvedBaseURL = this.normalizeEndpoint(
      provider,
      this.getApiEndpoint(provider) || config.baseURL || config.endpoint
    )

    if (!resolvedApiKey) {
      throw new Error(`${provider} API Key æœªè®¾ç½®`)
    }

    switch (provider) {
      case 'openai':
        this.clients.openai = new OpenAI({
          apiKey: resolvedApiKey,
          baseURL: resolvedBaseURL,
          dangerouslyAllowBrowser: true
        })
        break

      case 'anthropic':
        this.clients.anthropic = new Anthropic({
          apiKey: resolvedApiKey,
          baseURL: resolvedBaseURL,
          dangerouslyAllowBrowser: true
        })
        break

      case 'google':
        this.clients.google = new GoogleGenerativeAI(resolvedApiKey)
        break

      case 'vertex':
        // Vertex ä½¿ç”¨ RESTï¼Œæš‚ä¸åˆ›å»º SDK å®¢æˆ·ç«¯
        this.clients.vertex = { baseURL: resolvedBaseURL }
        break

      case 'custom':
        // è‡ªå®šä¹‰ API ä½¿ç”¨ fetch
        break

      default:
        throw new Error(`ä¸æ”¯æŒçš„æä¾›å•†: ${provider}`)
    }
  }

  // è§£æ Data URL
  extractBase64(dataUrl) {
    if (!dataUrl) return null
    const matches = dataUrl.match(/^data:([a-zA-Z0-9]+\/[a-zA-Z0-9-.+]+);base64,(.+)$/)
    if (!matches || matches.length !== 3) {
      return null
    }
    return {
      mimeType: matches[1],
      data: matches[2]
    }
  }

  // OpenAI æµå¼èŠå¤©
  async *streamChatOpenAI(messages, model, options = {}) {
    const client = this.clients.openai
    if (!client) throw new Error('OpenAI å®¢æˆ·ç«¯æœªåˆå§‹åŒ–')

    try {
      const formattedMessages = messages.map(msg => {
        // å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ„é€ å¤šæ¨¡æ€ content
        if (msg.images && msg.images.length > 0) {
          return {
            role: msg.role,
            content: [
              { type: 'text', text: msg.content },
              ...msg.images.map(imgUrl => ({
                type: 'image_url',
                image_url: {
                  url: imgUrl, // OpenAI ç›´æ¥æ”¯æŒ Data URL
                  detail: 'auto'
                }
              }))
            ]
          }
        }
        // çº¯æ–‡æœ¬æ¶ˆæ¯
        return {
          role: msg.role,
          content: msg.content
        }
      })

      const stream = await client.chat.completions.create({
        model,
        messages: formattedMessages,
        stream: true,
        temperature: options.temperature || 0.7,
        max_tokens: options.maxTokens || 4096
      })

      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content
        if (content) {
          yield { type: 'content', content }
        }

        if (chunk.choices[0]?.finish_reason) {
          yield { type: 'done', reason: chunk.choices[0].finish_reason }
        }
      }
    } catch (error) {
      console.error('OpenAI æµå¼è°ƒç”¨é”™è¯¯:', error)
      throw error
    }
  }

  // Anthropic æµå¼èŠå¤©
  async *streamChatAnthropic(messages, model, options = {}) {
    const client = this.clients.anthropic
    if (!client) throw new Error('Anthropic å®¢æˆ·ç«¯æœªåˆå§‹åŒ–')

    try {
      // åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œç”¨æˆ·æ¶ˆæ¯
      const systemMessage = messages.find(m => m.role === 'system')
      const chatMessages = messages.filter(m => m.role !== 'system')

      const formattedMessages = chatMessages.map(msg => {
        // æ˜ å°„è§’è‰²
        const role = msg.role === 'assistant' ? 'assistant' : 'user'

        // å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ„é€ å¤šæ¨¡æ€ content
        if (msg.images && msg.images.length > 0) {
          const imageBlocks = msg.images.map(imgUrl => {
            const extracted = this.extractBase64(imgUrl)
            if (!extracted) return null
            return {
              type: 'image',
              source: {
                type: 'base64',
                media_type: extracted.mimeType,
                data: extracted.data
              }
            }
          }).filter(Boolean)

          return {
            role,
            content: [
              ...imageBlocks, // å›¾ç‰‡æ”¾åœ¨å‰é¢é€šå¸¸æ•ˆæœæ›´å¥½
              { type: 'text', text: msg.content }
            ]
          }
        }

        // çº¯æ–‡æœ¬
        return {
          role,
          content: msg.content
        }
      })

      const stream = await client.messages.stream({
        model,
        max_tokens: options.maxTokens || 4096,
        temperature: options.temperature || 0.7,
        system: systemMessage?.content || undefined,
        messages: formattedMessages
      })

      for await (const event of stream) {
        if (event.type === 'content_block_delta' && event.delta?.type === 'text_delta') {
          yield { type: 'content', content: event.delta.text }
        }

        if (event.type === 'message_stop') {
          yield { type: 'done', reason: 'stop' }
        }
      }
    } catch (error) {
      console.error('Anthropic æµå¼è°ƒç”¨é”™è¯¯:', error)
      throw error
    }
  }

  // Google Gemini æµå¼èŠå¤©
  async *streamChatGoogle(messages, model, options = {}) {
    const client = this.clients.google
    if (!client) throw new Error('Google å®¢æˆ·ç«¯æœªåˆå§‹åŒ–')

    try {
      const temperature = options.temperature || 0.7
      const maxOutputTokens = options.maxTokens || 4096
      const lastMessage = messages[messages.length - 1]
      const prompt = lastMessage?.content || ''

      // å›¾ç‰‡/è§†é¢‘ç”Ÿæˆæ¨¡å¼åˆ¤æ–­
      // Google AI Studio æ”¯æŒï¼šImagen 4ã€Veo 3ã€Nano Banana Proã€Gemini Flash Image ç­‰
      const isImagenModel = model.toLowerCase().includes('imagen')
      const isVeoModel = model.toLowerCase().includes('veo')
      const isNanoBanana = model.toLowerCase().includes('nano') || model.includes('gemini-3-pro-image')
      // Gemini å›¾ç‰‡ç”Ÿæˆæ¨¡å‹ï¼šåŒ…å« 'image' æˆ– 'flash'
      const isGeminiImageModel = model.includes('gemini') && (model.includes('flash') || model.includes('image'))

      if (options.mode === 'image' || options.mode === 'video' || isImagenModel || isVeoModel || isNanoBanana || (options.imageParams && isGeminiImageModel)) {

        // Imagen/Veo ä½¿ç”¨ predict API (Google AI Studio ä¹Ÿæ”¯æŒ)
        if (isImagenModel || isVeoModel) {
          try {
            const apiKey = this.getApiKey('google')
            
            // å‡†å¤‡å‚æ•°
            const imageParams = options.imageParams || {}
            const videoParams = options.videoParams || {}
            
            // æ„å»º payload
            const instance = { prompt }
            // æ”¯æŒå‚è€ƒå›¾ (ä»…è§†é¢‘æ¨¡å¼)
            if (isVeoModel && videoParams.referenceImage) {
              instance.image = { bytesBase64Encoded: videoParams.referenceImage }
            }

            const parameters = {
              sampleCount: 1
            }

            // æ·»åŠ ç‰¹å®šå‚æ•°
            if (isImagenModel) {
              parameters.aspectRatio = imageParams.aspectRatio || '1:1'
            }
            if (isVeoModel) {
              parameters.aspectRatio = videoParams.aspectRatio || '16:9'
              if (videoParams.duration) {
                parameters.durationSeconds = parseInt(videoParams.duration, 10)
              }
              if (videoParams.withAudio) {
                parameters.includeAudio = true
              }
            }

            const response = await fetch(
              `https://generativelanguage.googleapis.com/v1beta/models/${model}:predict?key=${apiKey}`,
              {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                  instances: [instance],
                  parameters
                })
              }
            )

            if (!response.ok) {
              const errorText = await response.text()
              const endpoint = `https://generativelanguage.googleapis.com/v1beta/models/${model}:predict`
              throw new Error(`API é”™è¯¯ ${response.status} (${endpoint}): ${errorText}`)
            }

            const data = await response.json()

            if (data.predictions && data.predictions.length > 0) {
              const prediction = data.predictions[0]
              
              // å¤„ç†å›¾ç‰‡
              if (prediction.bytesBase64Encoded) {
                const mimeType = isVeoModel ? 'video/mp4' : 'image/png'
                const url = `data:${mimeType};base64,${prediction.bytesBase64Encoded}`
                const markdown = isVeoModel 
                  ? `<video controls src="${url}" width="100%"></video>`
                  : `![ç”Ÿæˆçš„å›¾ç‰‡](${url})`
                yield { type: 'content', content: markdown }
              } 
              // å¤„ç†è§†é¢‘ (Veo å¯èƒ½è¿”å› videoUri æˆ–å…¶ä»–å­—æ®µ)
              else if (prediction.videoUri) {
                 yield { type: 'content', content: `<video controls src="${prediction.videoUri}" width="100%"></video>\n\n[ä¸‹è½½è§†é¢‘](${prediction.videoUri})` }
              }
              else {
                yield { type: 'content', content: 'ç”ŸæˆæˆåŠŸä½†æœªè¯†åˆ«åˆ°è¿”å›æ•°æ®æ ¼å¼' }
              }
            } else {
              yield { type: 'content', content: 'ç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•' }
            }

            yield { type: 'done', reason: 'stop' }
            return
          } catch (err) {
            console.error('ç”Ÿæˆé”™è¯¯:', err)
            yield { type: 'content', content: `ç”Ÿæˆå¤±è´¥: ${err.message}\n\n**æ’æŸ¥å»ºè®®:**\n1. Veo/Imagen æ¨¡å‹åœ¨ AI Studio (API Key) ä¸Šå¯èƒ½å—é™æˆ–éœ€ç”³è¯·ç™½åå•ã€‚\n2. å°è¯•åˆ‡æ¢åˆ° **Google Vertex AI** æä¾›å•† (éœ€é…ç½® GCP é¡¹ç›®å’Œ OAuth)ã€‚\n3. æ£€æŸ¥ API Key æ˜¯å¦æœ‰æƒé™è®¿é—®è¯¥æ¨¡å‹ã€‚` }
            yield { type: 'done', reason: 'stop' }
            return
          }
        }

        // Nano Banana Pro å’Œ Gemini å›¾ç‰‡ç”Ÿæˆæ¨¡å‹ä½¿ç”¨ generateContent API
        if (isNanoBanana || isGeminiImageModel) {
          try {
            const apiKey = this.getApiKey('google')
            const imageParams = options.imageParams || {}

            // æ„å»º generationConfigï¼ŒåŒ…å« imageConfig
            const generationConfig = {
              responseModalities: ['TEXT', 'IMAGE'],
              // imageConfig ç”¨äºæ§åˆ¶å›¾ç‰‡ç”Ÿæˆå‚æ•°
              imageConfig: {
                aspectRatio: imageParams.aspectRatio || '1:1',
                imageSize: imageParams.resolution || '1K'  // æ”¯æŒ 1K, 2K, 4K
              }
            }

            const response = await fetch(
              `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`,
              {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                  contents: [{ parts: [{ text: prompt }] }],
                  generationConfig
                })
              }
            )

            if (!response.ok) {
              const errorText = await response.text()
              throw new Error(`API é”™è¯¯ ${response.status}: ${errorText}`)
            }

            const data = await response.json()
            const parts = data.candidates?.[0]?.content?.parts || []
            let hasImage = false
            for (const part of parts) {
              if (part.inlineData?.data) {
                const mimeType = part.inlineData.mimeType || 'image/png'
                const imageUrl = `data:${mimeType};base64,${part.inlineData.data}`
                yield { type: 'content', content: `![ç”Ÿæˆçš„å›¾ç‰‡](${imageUrl})\n\n` }
                hasImage = true
              } else if (part.text) {
                yield { type: 'content', content: part.text }
              }
            }
            if (!hasImage && parts.length === 0) {
              yield { type: 'content', content: 'å›¾ç‰‡ç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•' }
            }
            yield { type: 'done', reason: 'stop' }
            return
          } catch (imgError) {
            console.error('Gemini å›¾ç‰‡ç”Ÿæˆé”™è¯¯:', imgError)
            yield { type: 'content', content: `å›¾ç‰‡ç”Ÿæˆå¤±è´¥: ${imgError.message}` }
            yield { type: 'done', reason: 'stop' }
            return
          }
        }

        yield { type: 'content', content: `âš ï¸ å½“å‰æ¨¡å‹ \`${model}\` ä¸æ”¯æŒå›¾ç‰‡ç”Ÿæˆã€‚` }
        yield { type: 'done', reason: 'stop' }
        return
      }

      const genModel = client.getGenerativeModel({ model })

      // æ„å»ºå¤šæ¨¡æ€å†å²
      const history = messages.slice(0, -1).map(msg => {
        const parts = [{ text: msg.content }]
        if (msg.images && msg.images.length > 0) {
          msg.images.forEach(imgUrl => {
            const extracted = this.extractBase64(imgUrl)
            if (extracted) {
              parts.push({
                inlineData: {
                  mimeType: extracted.mimeType,
                  data: extracted.data
                }
              })
            }
          })
        }
        return {
          role: msg.role === 'assistant' ? 'model' : 'user',
          parts
        }
      })

      const chat = genModel.startChat({
        history,
        generationConfig: {
          temperature,
          maxOutputTokens
        }
      })

      // æ„å»ºå½“å‰å¤šæ¨¡æ€æ¶ˆæ¯
      let currentParts = [{ text: prompt }]
      if (lastMessage.images && lastMessage.images.length > 0) {
        lastMessage.images.forEach(imgUrl => {
          const extracted = this.extractBase64(imgUrl)
          if (extracted) {
            currentParts.push({
              inlineData: {
                mimeType: extracted.mimeType,
                data: extracted.data
              }
            })
          }
        })
      }

      const result = await chat.sendMessageStream(currentParts)

      for await (const chunk of result.stream) {
        const text = chunk.text()
        if (text) {
          yield { type: 'content', content: text }
        }
      }

      yield { type: 'done', reason: 'stop' }
    } catch (error) {
      console.error('Google æµå¼è°ƒç”¨é”™è¯¯:', error)
      throw error
    }
  }

  // è‡ªå®šä¹‰ API æµå¼èŠå¤©
  async *streamChatCustom(messages, model, config, options = {}) {
    const { baseURL, apiKey, corsProxyUrl } = config

    if (!baseURL) {
      throw new Error('è‡ªå®šä¹‰ API åœ°å€æœªè®¾ç½®')
    }

    // åº”ç”¨ CORS ä»£ç†ï¼ˆä¿æŒåè®®ï¼Œé˜²æ­¢ç¼ºå°‘æ–œæ ï¼‰
    const normalizedBase = this.normalizeEndpoint('custom', baseURL)
    const actualEndpoint = corsProxyUrl
      ? `${this.normalizeProxyUrl(corsProxyUrl)}${normalizedBase}`
      : normalizedBase

    try {
      const response = await fetch(`${actualEndpoint}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': apiKey ? `Bearer ${apiKey}` : ''
        },
        body: JSON.stringify({
          model,
          messages,
          stream: true,
          temperature: options.temperature || 0.7,
          max_tokens: options.maxTokens || 4096
        })
      })

      if (!response.ok) {
        throw new Error(`è‡ªå®šä¹‰ API è°ƒç”¨å¤±è´¥: ${response.statusText}`)
      }

      const reader = response.body.getReader()
      const decoder = new TextDecoder()

      while (true) {
        const { done, value } = await reader.read()
        if (done) break

        const chunk = decoder.decode(value)
        const lines = chunk.split('\n').filter(line => line.trim() !== '')

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6)
            if (data === '[DONE]') {
              yield { type: 'done', reason: 'stop' }
              return
            }

            try {
              const json = JSON.parse(data)
              const content = json.choices?.[0]?.delta?.content
              if (content) {
                yield { type: 'content', content }
              }
            } catch (e) {
              // å¿½ç•¥è§£æé”™è¯¯
            }
          }
        }
      }
    } catch (error) {
      console.error('è‡ªå®šä¹‰ API æµå¼è°ƒç”¨é”™è¯¯:', error)
      throw error
    }
  }

  /**
   * æµ‹è¯• API è¿æ¥
   * @param {string} provider - 'openai' | 'anthropic' | 'google' | 'custom'
   * @param {Object} config - { apiKey, endpoint, apiType }
   * @returns {Promise<Object>} { success, provider, responseTime, models, message, error }
   */
  async testConnection(provider, config = {}) {
    const mergedConfig = {
      ...config,
      apiKey: this.getApiKey(provider) || config.apiKey,
      endpoint: this.normalizeEndpoint(
        provider,
        config.endpoint || config.baseURL || this.getApiEndpoint(provider)
      )
    }
    const startTime = Date.now()

    try {
      let result

      switch (provider) {
        case 'openai':
          result = await this._testOpenAI(mergedConfig)
          break
        case 'anthropic':
          result = await this._testAnthropic(mergedConfig)
          break
        case 'google':
          result = await this._testGoogle(mergedConfig)
          break
        case 'vertex':
          result = await this._testVertex(mergedConfig)
          break
        case 'custom':
          result = await this._testCustom(mergedConfig)
          break
        default:
          throw new Error(`æœªçŸ¥çš„æä¾›å•†: ${provider}`)
      }

      return {
        success: true,
        provider,
        responseTime: Date.now() - startTime,
        ...result
      }
    } catch (error) {
      return {
        success: false,
        provider,
        responseTime: Date.now() - startTime,
        error: error.message,
        models: []
      }
    }
  }

  async _testOpenAI(config) {
    const endpoint = config.endpoint || 'https://api.openai.com/v1'
    const apiKey = config.apiKey

    if (!apiKey) throw new Error('è¯·æä¾› OpenAI API Key')

    // è·å–æ¨¡å‹åˆ—è¡¨
    const modelsResponse = await fetch(`${endpoint}/models`, {
      headers: { 'Authorization': `Bearer ${apiKey}` }
    })

    if (!modelsResponse.ok) {
      const error = await modelsResponse.text()
      throw new Error(`API é”™è¯¯: ${modelsResponse.status} - ${error}`)
    }

    const modelsData = await modelsResponse.json()
    const models = modelsData.data
      .map(m => ({ id: m.id, name: m.id, created: m.created }))
      .sort((a, b) => b.created - a.created)

    // æµ‹è¯•ç®€å•è°ƒç”¨
    const testResponse = await fetch(`${endpoint}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: 'test' }],
        max_tokens: 5
      })
    })

    if (!testResponse.ok) throw new Error('API è°ƒç”¨æµ‹è¯•å¤±è´¥')

    return {
      models,
      message: `æˆåŠŸè·å– ${models.length} ä¸ªæ¨¡å‹`
    }
  }

  async _testAnthropic(config) {
    const endpoint = config.endpoint || 'https://api.anthropic.com'
    const apiKey = config.apiKey

    if (!apiKey) throw new Error('è¯·æä¾› Anthropic API Key')

    // Anthropic æ²¡æœ‰å…¬å¼€çš„æ¨¡å‹åˆ—è¡¨ API
    const models = [
      { id: 'claude-3-5-sonnet-20241022', name: 'Claude 3.5 Sonnet' },
      { id: 'claude-3-5-haiku-20241022', name: 'Claude 3.5 Haiku' },
      { id: 'claude-3-opus-20240229', name: 'Claude 3 Opus' }
    ]

    // æµ‹è¯•è°ƒç”¨
    const testResponse = await fetch(`${endpoint}/v1/messages`, {
      method: 'POST',
      headers: {
        'x-api-key': apiKey,
        'anthropic-version': '2023-06-01',
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'claude-3-haiku-20240307',
        max_tokens: 10,
        messages: [{ role: 'user', content: 'test' }]
      })
    })

    if (!testResponse.ok) {
      const error = await testResponse.text()
      throw new Error(`API é”™è¯¯: ${testResponse.status} - ${error}`)
    }

    return {
      models,
      message: `è¿æ¥æˆåŠŸï¼Œå¯ç”¨ ${models.length} ä¸ªæ¨¡å‹`
    }
  }

  async _testGoogle(config) {
    const apiKey = config.apiKey

    if (!apiKey) throw new Error('è¯·æä¾› Google API Key')

    // è·å–æ¨¡å‹åˆ—è¡¨ï¼ˆå°Šé‡é…ç½®çš„ baseURLï¼Œé»˜è®¤ä½¿ç”¨ v1betaï¼‰
    const base = (config.endpoint || this.defaultEndpoints.google || '').replace(/\/+$/, '')
    const modelsEndpoint = `${base}/models?key=${apiKey}`
    const modelsResponse = await fetch(modelsEndpoint)

    if (!modelsResponse.ok) {
      const errorText = await modelsResponse.text()
      let errorMessage = `API é”™è¯¯: ${modelsResponse.status}`

      try {
        const errorJson = JSON.parse(errorText)
        // ç‰¹æ®Šå¤„ç†åœ°åŒºé™åˆ¶é”™è¯¯
        if (errorJson.error?.message?.includes('User location is not supported')) {
          errorMessage = 'è¯¥ API Key åœ¨å½“å‰åœ°åŒºä¸å¯ç”¨,è¯·å°è¯•ä½¿ç”¨ VPN æˆ–æ›´æ¢åœ°åŒº'
        } else {
          errorMessage += ` - ${errorJson.error?.message || errorText}`
        }
      } catch {
        errorMessage += ` - ${errorText}`
      }

      throw new Error(errorMessage)
    }

    const modelsData = await modelsResponse.json()
    const models = modelsData.models
      // ä¸è¿›è¡Œä»»ä½•è¿‡æ»¤ï¼Œä¿ç•™æ‰€æœ‰å¯ç”¨æ¨¡å‹
      .map(m => {
        // æ¨¡å‹ IDï¼šå»æ‰ 'models/' å‰ç¼€
        const modelId = m.name.replace('models/', '')
        // ä¼˜å…ˆä½¿ç”¨ API è¿”å›çš„å‹å¥½åç§° (displayName)ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ ID
        const displayName = m.displayName || modelId
        return {
          id: modelId,
          name: displayName
        }
      })
      .sort((a, b) => {
        // å®šä¹‰ä¼˜å…ˆçº§æƒé‡ï¼ˆè¶Šå°è¶Šé å‰ï¼‰
        const getPriority = (id) => {
          if (id === 'gemini-3-pro-preview') return 0 // ç»å¯¹ç½®é¡¶
          
          if (id.includes('veo')) return 2 // è§†é¢‘æ¨¡å‹ä¹Ÿå¾ˆé‡è¦
          
          // é™çº§ç‰¹å®šç±»å‹æ¨¡å‹
          if (id.includes('embedding') || id.includes('robotics') || id.includes('tts')) return 90
          
          // Gemini 3 ç³»åˆ—
          if (id.includes('gemini-3') && !id.includes('image')) return 10
          
          // Gemini 2.5 ç³»åˆ—
          if (id.includes('gemini-2.5-pro')) return 20
          if (id.includes('gemini-2.5-flash')) return 21
          if (id.includes('gemini-2.5')) return 22

          // å›¾ç‰‡ä¸“ç”¨æ¨¡å‹ (Nano Banana ç­‰)
          if (id.includes('image') || id.includes('imagen')) return 80 
          
          return 100 // å…¶ä»–
        }
        
        const priorityA = getPriority(a.id)
        const priorityB = getPriority(b.id)
        
        if (priorityA !== priorityB) {
          return priorityA - priorityB
        }
        
        return b.id.localeCompare(a.id)
      })

    // å¼ºåˆ¶æ·»åŠ  gemini-3-pro-preview (å¦‚æœ API æ²¡è¿”å›ï¼Œä½†æˆ‘ä»¬ç¡®ä¿¡å®ƒå¯ç”¨æˆ–æƒ³å°è¯•)
    const topModelId = 'gemini-3-pro-preview'
    if (!models.some(m => m.id === topModelId)) {
      models.unshift({
        id: topModelId,
        name: topModelId
      })
    }

    return {
      models,
      message: `æˆåŠŸè·å– ${models.length} ä¸ªæ¨¡å‹`
    }
  }

  async _testVertex(config) {
    const vertexConfig = this.getVertexConfig()
    const projectId = vertexConfig.projectId
    let location = vertexConfig.location || 'asia-southeast1'

    if (!projectId) {
      throw new Error('è¯·é…ç½® Vertex AI é¡¹ç›®ID (VITE_VERTEX_PROJECT_ID ç¯å¢ƒå˜é‡)')
    }

    // ä¼˜å…ˆä½¿ç”¨ OAuth Tokenï¼Œå›é€€åˆ° API Key
    let accessToken = googleAuthService.getAccessToken()
    const apiKey = config.apiKey

    console.log('[Vertex Test] OAuth Token:', accessToken ? `${accessToken.substring(0, 20)}...` : 'null')
    console.log('[Vertex Test] API Key:', apiKey ? 'å·²é…ç½®' : 'æœªé…ç½®')
    console.log('[Vertex Test] Project ID:', projectId)
    console.log('[Vertex Test] Location:', location)

    if (!accessToken && !apiKey) {
      throw new Error('è¯·å…ˆç‚¹å‡»"ä½¿ç”¨ Google è´¦æˆ·ç™»å½•"æŒ‰é’®è·å– OAuth Token')
    }

    // æ¨èçš„æ¨¡å‹åˆ—è¡¨ï¼ˆç”¨äºæä¾›å‹å¥½çš„åç§°å’Œæ’åºï¼‰
    const RECOMMENDED_MODELS = {
      'veo-3.0-generate-preview': { name: 'Veo 3.0 (è§†é¢‘ç”Ÿæˆ)', order: 10 },
      'veo-2.0-generate-001': { name: 'Veo 2.0 (è§†é¢‘ç”Ÿæˆ)', order: 11 },
      'imagen-3.0-generate-002': { name: 'Imagen 3.0 (å›¾ç‰‡ç”Ÿæˆ)', order: 20 },
      'imagen-3.0-fast-generate-001': { name: 'Imagen 3.0 Fast', order: 21 },
      'gemini-2.0-flash-001': { name: 'Gemini 2.0 Flash', order: 30 },
      'gemini-1.5-flash-001': { name: 'Gemini 1.5 Flash', order: 31 },
      'gemini-1.5-pro-001': { name: 'Gemini 1.5 Pro', order: 32 }
    }

    const headers = {
      'Content-Type': 'application/json'
    }

    // ä¼˜å…ˆä½¿ç”¨ OAuth Token
    if (accessToken) {
      headers['Authorization'] = `Bearer ${accessToken}`
    } else {
      headers['Authorization'] = `Bearer ${apiKey}`
    }

    // å®šä¹‰æµ‹è¯•å‡½æ•°
    const tryTest = async (testLocation) => {
      const endpoint = `https://${testLocation}-aiplatform.googleapis.com/v1/projects/${projectId}/locations/${testLocation}/publishers/google/models/gemini-1.5-flash-001:generateContent`
      console.log('[Vertex Test] Endpoint:', endpoint)
      return await fetch(endpoint, {
        method: 'POST',
        headers,
        body: JSON.stringify({
          contents: [{ role: 'user', parts: [{ text: 'test' }] }],
          generationConfig: { maxOutputTokens: 5 }
        })
      })
    }

    // ç¬¬ä¸€æ¬¡å°è¯•
    let testResponse = await tryTest(location)

    // å¦‚æœ 404 ä¸”å½“å‰ä¸æ˜¯ us-central1ï¼Œè‡ªåŠ¨é‡è¯• us-central1
    if (!testResponse.ok && testResponse.status === 404 && location !== 'us-central1') {
      console.warn(`[Vertex Test] Location ${location} failed with 404. Retrying with us-central1...`)
      const retryResponse = await tryTest('us-central1')
      testResponse = retryResponse
      if (testResponse.ok) {
        location = 'us-central1' // æ›´æ–°ä½ç½®ä»¥ä¾¿æ˜¾ç¤ºå’Œåç»­è°ƒç”¨
      }
    }

    if (!testResponse.ok) {
      const errorText = await testResponse.text()
      console.error('[Vertex Test] Error Response:', errorText)

      let errorMessage = `Vertex API é”™è¯¯ (${testResponse.status})`
      try {
        const errorJson = JSON.parse(errorText)
        const msg = errorJson.error?.message || errorText
        const code = errorJson.error?.code || testResponse.status

        // æ ¹æ®é”™è¯¯ç æä¾›æ›´å…·ä½“çš„æç¤º
        if (code === 401 || code === 403 || msg.includes('UNAUTHENTICATED') || msg.includes('PERMISSION_DENIED')) {
          if (msg.includes('OAuth')) {
            errorMessage = 'è®¤è¯å¤±è´¥: OAuth Token æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·é‡æ–°ç™»å½• Google è´¦æˆ·'
          } else if (msg.includes('PERMISSION_DENIED')) {
            errorMessage = `æƒé™ä¸è¶³: è¯·ç¡®ä¿:\n1. åœ¨ Google Cloud Console å¯ç”¨äº† Vertex AI API\n2. å½“å‰è´¦æˆ·æœ‰è®¿é—®é¡¹ç›® "${projectId}" çš„æƒé™\n3. OAuth åŒæ„å±å¹•å·²æ·»åŠ æ‚¨çš„è´¦æˆ·ä¸ºæµ‹è¯•ç”¨æˆ·`
          } else {
            errorMessage = `è®¤è¯å¤±è´¥ (${code}): ${msg}`
          }
        } else if (code === 404) {
          errorMessage = `é¡¹ç›®ä¸å­˜åœ¨æˆ–åŒºåŸŸä¸æ”¯æŒ: è¯·æ£€æŸ¥é¡¹ç›®ID "${projectId}" å’ŒåŒºåŸŸ "${location}" æ˜¯å¦æ­£ç¡®`
        } else {
          errorMessage += `: ${msg}`
        }
      } catch {
        errorMessage += `: ${errorText}`
      }
      throw new Error(errorMessage)
    }

    // è¿é€šæ€§æµ‹è¯•é€šè¿‡åï¼Œå°è¯•åŠ¨æ€è·å–æ¨¡å‹åˆ—è¡¨
    let models = []
    try {
      // ä»…è·å– Google å‘å¸ƒçš„æ¨¡å‹
      const listEndpoint = `https://${location}-aiplatform.googleapis.com/v1/projects/${projectId}/locations/${location}/publishers/google/models`
      console.log('[Vertex Test] Listing models from:', listEndpoint)
      
      const listResponse = await fetch(listEndpoint, {
        method: 'GET',
        headers
      })

      if (listResponse.ok) {
        const data = await listResponse.json()
        const fetchedModels = data.models || []
        
        // è¿‡æ»¤å¹¶æ˜ å°„æ¨¡å‹
        models = fetchedModels
          .filter(m => {
             const modelId = m.name.split('/').pop() //è·å–çŸ­ ID
             // åªä¿ç•™ gemini, imagen, veo ç³»åˆ—
             return /gemini|imagen|veo/i.test(modelId)
          })
          .map(m => {
            const fullId = m.name // e.g., projects/.../publishers/google/models/gemini-1.5-pro
            // Vertex è¿”å›çš„ name æ˜¯å…¨è·¯å¾„ï¼Œæˆ‘ä»¬éœ€è¦è½¬æˆ standard ID æ ¼å¼
            // é€šå¸¸æˆ‘ä»¬åªéœ€è¦ publishers/google/models/xxxx
            const shortId = m.name.split('/').pop()
            const standardId = `publishers/google/models/${shortId}`
            
            const recommended = RECOMMENDED_MODELS[shortId]
            
            return {
              id: standardId,
              name: recommended ? recommended.name : (m.displayName || shortId),
              order: recommended ? recommended.order : 999 // æ¨èçš„æ’å‰é¢ï¼Œå…¶ä»–çš„æ’åé¢
            }
          })
          
        // è¡¥å……æ¨èåˆ—è¡¨ä¸­æœ‰ä½† API æ²¡è¿”å›çš„ï¼ˆé˜²æ­¢ API åˆ—è¡¨ä¸å…¨æˆ–è¢«è¿‡æ»¤æ‰ï¼‰
        // æˆ–è€…ç®€å•èµ·è§ï¼Œå¦‚æœ API è°ƒç”¨æˆåŠŸï¼Œå°±ä»¥ API ä¸ºå‡†ï¼Ÿ
        // è€ƒè™‘åˆ°å…¼å®¹æ€§ï¼Œæœ€å¥½åˆå¹¶ã€‚
        Object.keys(RECOMMENDED_MODELS).forEach(key => {
          const standardId = `publishers/google/models/${key}`
          if (!models.find(m => m.id === standardId)) {
            models.push({
              id: standardId,
              name: RECOMMENDED_MODELS[key].name,
              order: RECOMMENDED_MODELS[key].order
            })
          }
        })
        
        // æ’åº
        models.sort((a, b) => a.order - b.order)

      } else {
        console.warn('[Vertex Test] Failed to list models, using fallback list.', listResponse.status)
        throw new Error('List failed') // ç”¨äºè§¦å‘ catch èµ° fallback
      }
    } catch (e) {
      console.warn('[Vertex Test] Fetch models failed, using default list.', e)
      // å›é€€åˆ°é»˜è®¤åˆ—è¡¨
      models = Object.keys(RECOMMENDED_MODELS).map(key => ({
        id: `publishers/google/models/${key}`,
        name: RECOMMENDED_MODELS[key].name
      })).sort((a, b) => {
         const orderA = RECOMMENDED_MODELS[a.id.split('/').pop()].order
         const orderB = RECOMMENDED_MODELS[b.id.split('/').pop()].order
         return orderA - orderB
      })
    }

    return {
      models,
      message: `Vertex AI è¿æ¥æˆåŠŸ (${accessToken ? 'OAuth Token' : 'API Key'})ï¼Œé¡¹ç›®: ${projectId}ï¼ŒåŒºåŸŸ: ${location}`
    }
  }

  async _testCustom(config) {
    const apiType = config.apiType || 'openai'
    const endpoint = config.endpoint || ''
    const apiKey = config.apiKey
    const corsProxyUrl = config.corsProxyUrl || ''

    if (!apiKey) throw new Error('è¯·æä¾› API Key')
    if (!endpoint) throw new Error('è¯·æä¾› API åœ°å€')

    // å¦‚æœé…ç½®äº† CORS ä»£ç†ï¼Œä½¿ç”¨ç”¨æˆ·æä¾›çš„ä»£ç†æœåŠ¡å™¨ï¼Œå¹¶è§„èŒƒåŒ–æ–œæ 
    const normalizedEndpoint = this.normalizeEndpoint('custom', endpoint)
    const actualEndpoint = corsProxyUrl
      ? `${this.normalizeProxyUrl(corsProxyUrl)}${normalizedEndpoint}`
      : normalizedEndpoint

    // æ ¹æ® API ç±»å‹è·¯ç”±åˆ°å¯¹åº”çš„æµ‹è¯•æ–¹æ³•
    const testConfig = {
      ...config,
      apiKey,
      endpoint: actualEndpoint
    }

    try {
      switch (apiType) {
        case 'openai':
          return await this._testOpenAI(testConfig)
        case 'anthropic':
          return await this._testAnthropic(testConfig)
        case 'google':
          return await this._testGoogle(testConfig)
        default:
          throw new Error(`ä¸æ”¯æŒçš„ API ç±»å‹: ${apiType}`)
      }
    } catch (error) {
      // å¦‚æœæ˜¯ fetch æœ¬èº«å¤±è´¥ï¼ˆç½‘ç»œé”™è¯¯ã€CORS ç­‰ï¼‰
      if (error.name === 'TypeError' && error.message.includes('fetch')) {
        throw new Error(`ç½‘ç»œè¯·æ±‚å¤±è´¥: ${error.message}ï¼ˆå¯èƒ½æ˜¯ CORS é—®é¢˜æˆ–ç½‘ç»œä¸å¯è¾¾ï¼‰`)
      }
      // é‡æ–°æŠ›å‡ºå…¶ä»–é”™è¯¯
      throw error
    }
  }

  // ç»Ÿä¸€çš„æµå¼èŠå¤©æ¥å£
  async *streamChat(provider, messages, model, config, options = {}) {
    // åˆå§‹åŒ–å®¢æˆ·ç«¯
    this.initClient(provider, config)

    switch (provider) {
      case 'openai':
        yield* this.streamChatOpenAI(messages, model, options)
        break

      case 'anthropic':
        yield* this.streamChatAnthropic(messages, model, options)
        break

      case 'google':
        yield* this.streamChatGoogle(messages, model, options)
        break

      case 'vertex':
        yield* this.streamChatVertex(messages, model, options)
        break

      case 'custom':
        yield* this.streamChatCustom(messages, model, config, options)
        break

      default:
        throw new Error(`ä¸æ”¯æŒçš„æä¾›å•†: ${provider}`)
    }
  }

  // Google Vertex AIï¼ˆRESTï¼‰æµå¼/éæµå¼
  async *streamChatVertex(messages, model, options = {}) {
    const { projectId, location } = this.getVertexConfig()

    if (!projectId) throw new Error('Vertex é¡¹ç›®IDæœªé…ç½®')

    // ä¼˜å…ˆä½¿ç”¨ OAuth Tokenï¼Œå›é€€åˆ° API Key
    let accessToken = googleAuthService.getAccessToken()
    const apiKey = this.getApiKey('vertex')

    if (!accessToken && !apiKey) {
      throw new Error('è¯·ç™»å½• Google è´¦æˆ·æˆ–æä¾› Vertex API Key')
    }

    const lastMessage = messages[messages.length - 1]
    const prompt = lastMessage?.content || ''

    // æ£€æŸ¥æ˜¯å¦æ˜¯ Veo è§†é¢‘ç”Ÿæˆæ¨¡å‹
    const isVeoModel = model.includes('veo')
    // æ£€æŸ¥æ˜¯å¦æ˜¯ Imagen å›¾ç‰‡ç”Ÿæˆæ¨¡å‹
    const isImagenModel = model.includes('imagen')

    // Veo è§†é¢‘ç”Ÿæˆ
    if (isVeoModel) {
      yield* this._generateVertexVideo(projectId, location, model, prompt, accessToken || apiKey, options.videoParams)
      return
    }

    // Imagen å›¾ç‰‡ç”Ÿæˆ
    if (isImagenModel) {
      yield* this._generateVertexImage(projectId, location, model, prompt, accessToken || apiKey, options.imageParams)
      return
    }

    // Gemini å¯¹è¯æ¨¡å‹
    const temperature = options.temperature || 0.7
    const maxOutputTokens = options.maxTokens || 2048
    const history = messages.slice(0, -1).map(msg => ({
      role: msg.role === 'assistant' ? 'model' : 'user',
      parts: [{ text: msg.content }]
    }))

    // æ‹¼æ¥ endpoint
    const baseURL = `https://${location}-aiplatform.googleapis.com/v1`
    const modelPath = `projects/${projectId}/locations/${location}/${model}`
    const url = `${baseURL}/${modelPath}:streamGenerateContent?alt=sse`

    const headers = {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${accessToken || apiKey}`
    }

    const body = {
      contents: [
        ...history,
        { role: 'user', parts: [{ text: prompt }] }
      ],
      generationConfig: {
        temperature,
        maxOutputTokens
      }
    }

    const response = await fetch(url, {
      method: 'POST',
      headers,
      body: JSON.stringify(body)
    })

    if (!response.ok || !response.body) {
      const errText = await response.text()
      throw new Error(`Vertex è¯·æ±‚å¤±è´¥: ${response.status} ${errText}`)
    }

    const reader = response.body.getReader()
    const decoder = new TextDecoder()
    let buffer = ''

    while (true) {
      const { done, value } = await reader.read()
      if (done) break
      buffer += decoder.decode(value, { stream: true })
      const parts = buffer.split('\n')
      buffer = parts.pop() || ''
      for (const line of parts) {
        const trimmed = line.replace(/^data:\s*/, '').trim()
        if (!trimmed) continue
        if (trimmed === '[DONE]') {
          yield { type: 'done', reason: 'stop' }
          return
        }
        try {
          const json = JSON.parse(trimmed)
          const text = json?.candidates?.[0]?.content?.parts?.map(p => p.text).join('') || ''
          if (text) {
            yield { type: 'content', content: text }
          }
        } catch (e) {
          console.warn('Vertex SSE parse error', e)
        }
      }
    }

    yield { type: 'done', reason: 'stop' }
  }

  // Vertex AI Veo è§†é¢‘ç”Ÿæˆ
  async *_generateVertexVideo(projectId, location, model, prompt, token, videoParams = {}) {
    yield { type: 'content', content: 'ğŸ¬ æ­£åœ¨ç”Ÿæˆè§†é¢‘ï¼Œè¯·ç¨å€™...\n\n' }

    const modelName = model.replace('publishers/google/models/', '')
    const endpoint = `https://${location}-aiplatform.googleapis.com/v1/projects/${projectId}/locations/${location}/publishers/google/models/${modelName}:predict`

    try {
      const instance = { prompt }
      // å¦‚æœæœ‰å‚è€ƒå›¾ï¼Œæ·»åŠ åˆ° payload
      if (videoParams.referenceImage) {
        instance.image = { bytesBase64Encoded: videoParams.referenceImage }
      }

      const response = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${token}`
        },
        body: JSON.stringify({
          instances: [instance],
          parameters: {
            sampleCount: 1,
            aspectRatio: videoParams.aspectRatio || '16:9',
            durationSeconds: parseInt(videoParams.duration || '5', 10),
            includeAudio: !!videoParams.withAudio
          }
        })
      })

      if (!response.ok) {
        const errorText = await response.text()
        console.error(`Vertex Video Gen Failed. Endpoint: ${endpoint}, Status: ${response.status}, Error: ${errorText}`)
        yield { type: 'content', content: `è§†é¢‘ç”Ÿæˆå¤±è´¥ (Vertex): ${response.status} - ${errorText}\n\nè¯·æ±‚åœ°å€: \`${endpoint}\`` }
        yield { type: 'done', reason: 'error' }
        return
      }

      const data = await response.json()

      if (data.predictions && data.predictions.length > 0) {
        const prediction = data.predictions[0]

        if (prediction.bytesBase64Encoded) {
          const videoUrl = `data:video/mp4;base64,${prediction.bytesBase64Encoded}`
          yield { type: 'content', content: `<video controls src="${videoUrl}" width="100%" style="max-width: 640px; border-radius: 8px;"></video>\n\n` }
        } else if (prediction.videoUri) {
          yield { type: 'content', content: `<video controls src="${prediction.videoUri}" width="100%" style="max-width: 640px; border-radius: 8px;"></video>\n\n[ä¸‹è½½è§†é¢‘](${prediction.videoUri})` }
        } else {
          yield { type: 'content', content: 'è§†é¢‘ç”ŸæˆæˆåŠŸä½†æœªè¯†åˆ«åˆ°è¿”å›æ•°æ®æ ¼å¼' }
        }
      } else {
        yield { type: 'content', content: 'è§†é¢‘ç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•' }
      }
    } catch (error) {
      yield { type: 'content', content: `è§†é¢‘ç”Ÿæˆé”™è¯¯: ${error.message}` }
    }

    yield { type: 'done', reason: 'stop' }
  }

  // Vertex AI Imagen å›¾ç‰‡ç”Ÿæˆ
  async *_generateVertexImage(projectId, location, model, prompt, token, imageParams = {}) {
    yield { type: 'content', content: 'ğŸ–¼ï¸ æ­£åœ¨ç”Ÿæˆå›¾ç‰‡ï¼Œè¯·ç¨å€™...\n\n' }

    const modelName = model.replace('publishers/google/models/', '')
    const endpoint = `https://${location}-aiplatform.googleapis.com/v1/projects/${projectId}/locations/${location}/publishers/google/models/${modelName}:predict`

    try {
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${token}`
        },
        body: JSON.stringify({
          instances: [{ prompt }],
          parameters: {
            sampleCount: 1,
            aspectRatio: imageParams.aspectRatio || '1:1'
          }
        })
      })

      if (!response.ok) {
        const errorText = await response.text()
        yield { type: 'content', content: `å›¾ç‰‡ç”Ÿæˆå¤±è´¥: ${response.status} - ${errorText}` }
        yield { type: 'done', reason: 'error' }
        return
      }

      const data = await response.json()

      if (data.predictions && data.predictions.length > 0) {
        const prediction = data.predictions[0]

        if (prediction.bytesBase64Encoded) {
          const imageUrl = `data:image/png;base64,${prediction.bytesBase64Encoded}`
          yield { type: 'content', content: `![ç”Ÿæˆçš„å›¾ç‰‡](${imageUrl})\n\n` }
        } else {
          yield { type: 'content', content: 'å›¾ç‰‡ç”ŸæˆæˆåŠŸä½†æœªè¯†åˆ«åˆ°è¿”å›æ•°æ®æ ¼å¼' }
        }
      } else {
        yield { type: 'content', content: 'å›¾ç‰‡ç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•' }
      }
    } catch (error) {
      yield { type: 'content', content: `å›¾ç‰‡ç”Ÿæˆé”™è¯¯: ${error.message}` }
    }

    yield { type: 'done', reason: 'stop' }
  }

  // éæµå¼èŠå¤©ï¼ˆå…¼å®¹æ¥å£ï¼‰
  async chat(provider, messages, model, config, options = {}) {
    let fullContent = ''

    for await (const chunk of this.streamChat(provider, messages, model, config, options)) {
      if (chunk.type === 'content') {
        fullContent += chunk.content
      }
    }

    return {
      content: fullContent,
      role: 'assistant'
    }
  }
}

// å¯¼å‡ºå•ä¾‹
const aiService = new AIService()
export default aiService
